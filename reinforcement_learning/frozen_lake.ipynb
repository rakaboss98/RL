{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initialised quality table is \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(\"The initialised quality table is \\n {}\".format(q_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_episodes = 100000\n",
    "max_steps_per_episode = 100\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "exploration_rate=1\n",
    "max_exploration_rate=1\n",
    "min_exploration_rate=0.01\n",
    "exploration_rate_decay=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rewards for 0 thousand episodes are 0.05500000000000004\n",
      "The rewards for 1 thousand episodes are 0.20200000000000015\n",
      "The rewards for 2 thousand episodes are 0.3960000000000003\n",
      "The rewards for 3 thousand episodes are 0.5550000000000004\n",
      "The rewards for 4 thousand episodes are 0.6180000000000004\n",
      "The rewards for 5 thousand episodes are 0.6550000000000005\n",
      "The rewards for 6 thousand episodes are 0.6640000000000005\n",
      "The rewards for 7 thousand episodes are 0.6800000000000005\n",
      "The rewards for 8 thousand episodes are 0.6880000000000005\n",
      "The rewards for 9 thousand episodes are 0.6930000000000005\n",
      "The rewards for 10 thousand episodes are 0.6640000000000005\n",
      "The rewards for 11 thousand episodes are 0.6940000000000005\n",
      "The rewards for 12 thousand episodes are 0.6730000000000005\n",
      "The rewards for 13 thousand episodes are 0.6550000000000005\n",
      "The rewards for 14 thousand episodes are 0.6870000000000005\n",
      "The rewards for 15 thousand episodes are 0.6610000000000005\n",
      "The rewards for 16 thousand episodes are 0.6850000000000005\n",
      "The rewards for 17 thousand episodes are 0.6880000000000005\n",
      "The rewards for 18 thousand episodes are 0.7180000000000005\n",
      "The rewards for 19 thousand episodes are 0.6950000000000005\n",
      "The rewards for 20 thousand episodes are 0.6930000000000005\n",
      "The rewards for 21 thousand episodes are 0.6790000000000005\n",
      "The rewards for 22 thousand episodes are 0.6760000000000005\n",
      "The rewards for 23 thousand episodes are 0.6920000000000005\n",
      "The rewards for 24 thousand episodes are 0.6960000000000005\n",
      "The rewards for 25 thousand episodes are 0.7010000000000005\n",
      "The rewards for 26 thousand episodes are 0.6820000000000005\n",
      "The rewards for 27 thousand episodes are 0.6810000000000005\n",
      "The rewards for 28 thousand episodes are 0.6980000000000005\n",
      "The rewards for 29 thousand episodes are 0.6780000000000005\n",
      "The rewards for 30 thousand episodes are 0.7080000000000005\n",
      "The rewards for 31 thousand episodes are 0.6980000000000005\n",
      "The rewards for 32 thousand episodes are 0.7000000000000005\n",
      "The rewards for 33 thousand episodes are 0.6800000000000005\n",
      "The rewards for 34 thousand episodes are 0.6660000000000005\n",
      "The rewards for 35 thousand episodes are 0.7310000000000005\n",
      "The rewards for 36 thousand episodes are 0.6760000000000005\n",
      "The rewards for 37 thousand episodes are 0.6700000000000005\n",
      "The rewards for 38 thousand episodes are 0.6770000000000005\n",
      "The rewards for 39 thousand episodes are 0.7120000000000005\n",
      "The rewards for 40 thousand episodes are 0.7270000000000005\n",
      "The rewards for 41 thousand episodes are 0.6590000000000005\n",
      "The rewards for 42 thousand episodes are 0.6760000000000005\n",
      "The rewards for 43 thousand episodes are 0.6740000000000005\n",
      "The rewards for 44 thousand episodes are 0.6970000000000005\n",
      "The rewards for 45 thousand episodes are 0.6870000000000005\n",
      "The rewards for 46 thousand episodes are 0.6370000000000005\n",
      "The rewards for 47 thousand episodes are 0.6640000000000005\n",
      "The rewards for 48 thousand episodes are 0.6940000000000005\n",
      "The rewards for 49 thousand episodes are 0.6900000000000005\n",
      "The rewards for 50 thousand episodes are 0.6610000000000005\n",
      "The rewards for 51 thousand episodes are 0.6560000000000005\n",
      "The rewards for 52 thousand episodes are 0.7040000000000005\n",
      "The rewards for 53 thousand episodes are 0.6930000000000005\n",
      "The rewards for 54 thousand episodes are 0.6720000000000005\n",
      "The rewards for 55 thousand episodes are 0.6920000000000005\n",
      "The rewards for 56 thousand episodes are 0.6550000000000005\n",
      "The rewards for 57 thousand episodes are 0.6920000000000005\n",
      "The rewards for 58 thousand episodes are 0.6680000000000005\n",
      "The rewards for 59 thousand episodes are 0.6530000000000005\n",
      "The rewards for 60 thousand episodes are 0.6870000000000005\n",
      "The rewards for 61 thousand episodes are 0.6830000000000005\n",
      "The rewards for 62 thousand episodes are 0.6760000000000005\n",
      "The rewards for 63 thousand episodes are 0.6810000000000005\n",
      "The rewards for 64 thousand episodes are 0.6760000000000005\n",
      "The rewards for 65 thousand episodes are 0.6340000000000005\n",
      "The rewards for 66 thousand episodes are 0.6870000000000005\n",
      "The rewards for 67 thousand episodes are 0.7120000000000005\n",
      "The rewards for 68 thousand episodes are 0.6750000000000005\n",
      "The rewards for 69 thousand episodes are 0.6880000000000005\n",
      "The rewards for 70 thousand episodes are 0.6620000000000005\n",
      "The rewards for 71 thousand episodes are 0.6670000000000005\n",
      "The rewards for 72 thousand episodes are 0.6990000000000005\n",
      "The rewards for 73 thousand episodes are 0.6950000000000005\n",
      "The rewards for 74 thousand episodes are 0.7090000000000005\n",
      "The rewards for 75 thousand episodes are 0.6490000000000005\n",
      "The rewards for 76 thousand episodes are 0.7010000000000005\n",
      "The rewards for 77 thousand episodes are 0.6750000000000005\n",
      "The rewards for 78 thousand episodes are 0.6490000000000005\n",
      "The rewards for 79 thousand episodes are 0.7010000000000005\n",
      "The rewards for 80 thousand episodes are 0.6730000000000005\n",
      "The rewards for 81 thousand episodes are 0.6880000000000005\n",
      "The rewards for 82 thousand episodes are 0.6930000000000005\n",
      "The rewards for 83 thousand episodes are 0.6990000000000005\n",
      "The rewards for 84 thousand episodes are 0.6570000000000005\n",
      "The rewards for 85 thousand episodes are 0.6630000000000005\n",
      "The rewards for 86 thousand episodes are 0.7060000000000005\n",
      "The rewards for 87 thousand episodes are 0.6820000000000005\n",
      "The rewards for 88 thousand episodes are 0.6650000000000005\n",
      "The rewards for 89 thousand episodes are 0.6900000000000005\n",
      "The rewards for 90 thousand episodes are 0.6750000000000005\n",
      "The rewards for 91 thousand episodes are 0.7090000000000005\n",
      "The rewards for 92 thousand episodes are 0.6780000000000005\n",
      "The rewards for 93 thousand episodes are 0.7060000000000005\n",
      "The rewards for 94 thousand episodes are 0.6760000000000005\n",
      "The rewards for 95 thousand episodes are 0.7050000000000005\n",
      "The rewards for 96 thousand episodes are 0.6690000000000005\n",
      "The rewards for 97 thousand episodes are 0.7080000000000005\n",
      "The rewards for 98 thousand episodes are 0.6890000000000005\n",
      "The rewards for 99 thousand episodes are 0.6960000000000005\n",
      "\n",
      " *** The q table after 10000 episodes is *** \n",
      "[[0.58248464 0.52412158 0.52116707 0.52057691]\n",
      " [0.40683115 0.30286481 0.3616416  0.51022213]\n",
      " [0.42527188 0.41224505 0.41961639 0.47582096]\n",
      " [0.23704588 0.33169033 0.22545084 0.45917808]\n",
      " [0.60128676 0.47458676 0.34152692 0.35351904]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.35054246 0.14751587 0.17353462 0.11812127]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.35037272 0.53555955 0.37094586 0.64429685]\n",
      " [0.31617845 0.68548414 0.51034736 0.49413578]\n",
      " [0.62449416 0.38609068 0.36650457 0.26883106]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.56906156 0.56683456 0.78935879 0.33473549]\n",
      " [0.71640665 0.94345621 0.75576394 0.73710791]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Q-learning algorithm\n",
    "reward_all_episodes = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    for step in range(max_steps_per_episode):\n",
    "        explr_th = random.uniform(0,1)\n",
    "        if explr_th > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q table\n",
    "\n",
    "        q_table[state, action] = q_table[state, action]*(1-learning_rate) + \\\n",
    "            learning_rate*(reward+discount_rate*np.max(q_table[new_state,:]))\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update the Exploration rate\n",
    "    exploration_rate = min_exploration_rate +\\\n",
    "                       (max_exploration_rate-min_exploration_rate)*np.exp(-exploration_rate_decay*episode)\n",
    "\n",
    "    # Append the episode reward to total reward\n",
    "    reward_all_episodes.append(episode_reward)\n",
    "\n",
    "# Calculate and print the rewards per thousand episodes\n",
    "rewards_per_thousand  = np.split(np.array(reward_all_episodes), num_episodes/1000)\n",
    "\n",
    "for i, r  in enumerate(rewards_per_thousand):\n",
    "    print(\"The rewards for {} thousand episodes are {}\".format(i, sum(r/1000)))\n",
    "\n",
    "print(\"\\n *** The q table after 10000 episodes is *** \\n{}\".format(q_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***You've reached the goal***\n"
     ]
    }
   ],
   "source": [
    "# Visualise the Agent play the game\n",
    "env.reset()\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    time.sleep(1)\n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward==1:\n",
    "                print(\"***You've reached the goal***\")\n",
    "            else:\n",
    "                print(\"***You've fell through hole***\")\n",
    "                time.sleep(0.3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
